# ==========================================
# Local Development Environment Variables
# ==========================================
# Copy this file to .env and fill in your values

# ==========================================
# External API Keys
# ==========================================

# GitLab
GITLAB_TOKEN=your_gitlab_personal_access_token
GITLAB_BASE_URL=https://gitlab.com/api/v4

# Slack
SLACK_BOT_TOKEN=xoxb-your-slack-bot-token
SLACK_SIGNING_SECRET=your_slack_signing_secret

# Backlog
BACKLOG_API_KEY=your_backlog_api_key
BACKLOG_SPACE_URL=https://your_space.backlog.com

# ==========================================
# LocalStack Configuration
# ==========================================
LOCALSTACK_ENDPOINT=http://localhost:4566
AWS_DEFAULT_REGION=ap-southeast-1
AWS_ACCESS_KEY_ID=test
AWS_SECRET_ACCESS_KEY=test

# LocalStack Pro/Desktop (với auth key)
LOCALSTACK_AUTH_TOKEN=ls-siWACAyO-9014-qeMI-3043-qojItIhide54  # Thay bằng auth key của bạn
LOCALSTACK_API_KEY=ls-siWACAyO-9014-qeMI-3043-qojItIhide54     # Same as AUTH_TOKEN

# ==========================================
# Model Configuration
# ==========================================

# Embedding Model
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
EMBEDDING_DEVICE=cpu  # Options: cpu, cuda, mps (Mac M1/M2)
EMBEDDING_BATCH_SIZE=32

# LLM Model
LLM_MODEL=stabilityai/japanese-stablelm-instruct-alpha-7b-v2
LLM_DEVICE=cpu  # Options: cpu, cuda
LLM_LOAD_IN_4BIT=true  # Set to false if you have enough VRAM
LLM_MAX_NEW_TOKENS=512
LLM_TEMPERATURE=0.7
LLM_TOP_P=0.9

# ==========================================
# Vector Store Configuration
# ==========================================
VECTOR_STORE=chromadb  # Options: chromadb, faiss
CHROMADB_HOST=localhost
CHROMADB_PORT=8001
CHROMADB_COLLECTION=chatbot_knowledge

# FAISS Configuration (if using FAISS)
FAISS_INDEX_PATH=./data/faiss_index
FAISS_DIMENSION=384

# ==========================================
# Database Configuration
# ==========================================
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=chatbot
POSTGRES_USER=chatbot
POSTGRES_PASSWORD=chatbot123

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_TTL=3600  # Cache TTL in seconds

# ==========================================
# Application Configuration
# ==========================================
APP_HOST=0.0.0.0
APP_PORT=8000
DEBUG=true
LOG_LEVEL=INFO  # Options: DEBUG, INFO, WARNING, ERROR

# Session
SESSION_TIMEOUT=1800  # 30 minutes

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_PER_MINUTE=60

# ==========================================
# Services URLs (Docker internal)
# ==========================================
EMBEDDING_SERVICE_URL=http://localhost:8002
LLM_SERVICE_URL=http://localhost:8003

# ==========================================
# Performance Tuning
# ==========================================

# Max concurrent requests
MAX_WORKERS=4

# Timeout settings
REQUEST_TIMEOUT=300  # 5 minutes
LLM_TIMEOUT=120  # 2 minutes
EMBEDDING_TIMEOUT=30  # 30 seconds

# ==========================================
# Feature Flags
# ==========================================
ENABLE_CACHING=true
ENABLE_TELEMETRY=false
ENABLE_CONVERSATION_HISTORY=true
ENABLE_AUTHENTICATION=false  # Set to true for production

# ==========================================
# Development Helpers
# ==========================================
RELOAD_ON_CHANGE=true
MOCK_EXTERNAL_APIS=false  # Set to true to use mock data
