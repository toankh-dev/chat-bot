# ==========================================
# Local Development Environment Variables
# ==========================================
# Copy this file to .env and fill in your values

# ==========================================
# External API Keys
# ==========================================

# ========== OPTION 1: DISCORD (RECOMMENDED!) ==========
# FREE and EASY! Only takes 5 minutes to setup
# See DISCORD_SETUP_GUIDE.md for step-by-step instructions

DISCORD_BOT_TOKEN=your_discord_bot_token_here
DISCORD_APPLICATION_ID=your_discord_app_id_here
DISCORD_GUILD_ID=your_discord_server_id_here
DISCORD_CHANNEL_IDS=channel_id_1,channel_id_2  # Comma-separated

# Optional: Specific channels
DISCORD_NOTIFICATION_CHANNEL=your_notification_channel_id
DISCORD_ISSUES_CHANNEL=your_issues_channel_id

# ========== OPTION 2: GitLab/Slack/Backlog (OPTIONAL) ==========
# Only if you have these services
# Can leave empty if using Discord

# GitLab (OPTIONAL)
GITLAB_TOKEN=your_gitlab_personal_access_token
GITLAB_BASE_URL=https://gitlab.com/api/v4

# Slack (OPTIONAL)
SLACK_BOT_TOKEN=xoxb-your-slack-bot-token
SLACK_SIGNING_SECRET=your_slack_signing_secret

# Backlog (OPTIONAL)
BACKLOG_API_KEY=your_backlog_api_key
BACKLOG_SPACE_URL=https://your_space.backlog.com

# ========== DATA SOURCE SELECTION ==========
USE_DISCORD=true   # Set to true to use Discord
USE_GITLAB=false   # Set to true if you have GitLab token
USE_SLACK=false    # Set to true if you have Slack token
USE_BACKLOG=false  # Set to true if you have Backlog token

# ==========================================
# LocalStack Configuration
# ==========================================
LOCALSTACK_ENDPOINT=http://localhost:4566
AWS_DEFAULT_REGION=ap-southeast-1
AWS_ACCESS_KEY_ID=test
AWS_SECRET_ACCESS_KEY=test

# LocalStack Pro/Desktop (vá»›i auth key)
LOCALSTACK_AUTH_TOKEN=
LOCALSTACK_API_KEY=

# ==========================================
# Model Configuration
# ==========================================

# Embedding Configuration
# Choose embedding provider: local or voyageai (recommended)
EMBEDDING_PROVIDER=voyageai  # Options: local, voyageai

# VoyageAI Embedding (RECOMMENDED - High quality embeddings via API)
# Get your API key from: https://www.voyageai.com/
VOYAGE_API_KEY=your_voyage_api_key_here
VOYAGE_MODEL=voyage-2  # Options: voyage-2, voyage-code-2, voyage-large-2

# Local Embedding Model (if EMBEDDING_PROVIDER=local)
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
EMBEDDING_DEVICE=cpu  # Options: cpu, cuda, mps (Mac M1/M2)
EMBEDDING_BATCH_SIZE=32

# LLM Configuration
# Choose one: local, openai, or claude
LLM_PROVIDER=groq  # Options: local, openai, claude, groq

# For Local LLM (if LLM_PROVIDER=local) - Slow on CPU
LLM_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0
LLM_DEVICE=cpu  # Options: cpu, cuda
LLM_LOAD_IN_4BIT=false
LLM_MAX_NEW_TOKENS=128
LLM_TEMPERATURE=0.7
LLM_TOP_P=0.9

# ==========================================
# Vector Store Configuration
# ==========================================
VECTOR_STORE=chromadb  # Options: chromadb, faiss
CHROMADB_HOST=localhost
CHROMADB_PORT=8001
CHROMADB_COLLECTION=chatbot_knowledge

# FAISS Configuration (if using FAISS)
FAISS_INDEX_PATH=./data/faiss_index
FAISS_DIMENSION=384

# ==========================================
# Database Configuration
# ==========================================
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=chatbot
POSTGRES_USER=chatbot
POSTGRES_PASSWORD=chatbot123

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_TTL=3600  # Cache TTL in seconds

# ==========================================
# Application Configuration
# ==========================================
APP_HOST=0.0.0.0
APP_PORT=8000
DEBUG=true
LOG_LEVEL=INFO  # Options: DEBUG, INFO, WARNING, ERROR

# Session
SESSION_TIMEOUT=1800  # 30 minutes

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_PER_MINUTE=60

# ==========================================
# Services URLs (Docker internal)
# ==========================================
# EMBEDDING_SERVICE_URL=http://localhost:8002  # Disabled - Using VoyageAI
LLM_SERVICE_URL=http://localhost:8003

# ==========================================
# Performance Tuning
# ==========================================

# Max concurrent requests
MAX_WORKERS=4

# Timeout settings
REQUEST_TIMEOUT=300  # 5 minutes
LLM_TIMEOUT=120  # 2 minutes
EMBEDDING_TIMEOUT=30  # 30 seconds

# ==========================================
# Feature Flags
# ==========================================
ENABLE_CACHING=true
ENABLE_TELEMETRY=false
ENABLE_CONVERSATION_HISTORY=true
ENABLE_AUTHENTICATION=false  # Set to true for production

# ==========================================
# Development Helpers
# ==========================================
RELOAD_ON_CHANGE=true
MOCK_EXTERNAL_APIS=false  # Set to true to use mock data
