# üöÄ H∆∞·ªõng D·∫´n Setup Local v·ªõi HuggingFace + LangChain + LocalStack

## üìã Y√™u C·∫ßu H·ªá Th·ªëng

### Minimum (CPU Only)
- **RAM**: 16GB
- **Storage**: 50GB free space
- **CPU**: 8 cores (Intel i7/AMD Ryzen 7 ho·∫∑c t·ªët h∆°n)
- **Response time**: 10-30 gi√¢y

### Recommended (v·ªõi GPU)
- **RAM**: 32GB
- **GPU**: NVIDIA GPU v·ªõi 12GB+ VRAM (RTX 3080/4070 tr·ªü l√™n)
- **Storage**: 100GB SSD
- **CPU**: 8+ cores
- **Response time**: 3-10 gi√¢y

### Software Requirements
- Docker Desktop (version 24.0+)
- Docker Compose (version 2.20+)
- Python 3.11+
- Git

---

## üì• B∆∞·ªõc 1: Clone v√† Chu·∫©n B·ªã

```bash
# Clone repository
cd c:\Users\toankh\Documents\chat-bot

# Create .env file from example
cp .env.example .env

# Edit .env and fill in your API keys
notepad .env
```

### C·∫•u h√¨nh quan tr·ªçng trong .env:

```bash
# External API Keys (B·∫ÆT BU·ªòC)
GITLAB_TOKEN=your_gitlab_token_here
SLACK_BOT_TOKEN=xoxb-your-slack-token-here
BACKLOG_API_KEY=your_backlog_key_here

# Model Configuration
EMBEDDING_DEVICE=cpu  # ƒê·ªïi th√†nh 'cuda' n·∫øu c√≥ GPU
LLM_DEVICE=cpu        # ƒê·ªïi th√†nh 'cuda' n·∫øu c√≥ GPU
LLM_LOAD_IN_4BIT=true # false n·∫øu c√≥ ƒë·ªß 24GB+ VRAM
```

---

## üê≥ B∆∞·ªõc 2: Start Services v·ªõi Docker Compose

### Option A: Start t·∫•t c·∫£ services c√πng l√∫c

```bash
# Start all services
docker-compose up -d

# Xem logs
docker-compose logs -f
```

### Option B: Start t·ª´ng service m·ªôt (Recommended cho l·∫ßn ƒë·∫ßu)

```bash
# 1. Start LocalStack first
docker-compose up -d localstack
docker-compose logs -f localstack

# 2. Start ChromaDB
docker-compose up -d chromadb
docker-compose logs -f chromadb

# 3. Start Embedding Service (s·∫Ω download model ~500MB, m·∫•t 2-5 ph√∫t)
docker-compose up -d embedding-service
docker-compose logs -f embedding-service

# 4. Start LLM Service (s·∫Ω download model ~14GB, m·∫•t 10-30 ph√∫t)
docker-compose up -d llm-service
docker-compose logs -f llm-service

# 5. Start PostgreSQL & Redis
docker-compose up -d postgres redis

# 6. Start Main Application
docker-compose up -d app
docker-compose logs -f app
```

---

## ‚è≥ B∆∞·ªõc 3: Ch·ªù Models Download

**QUAN TR·ªåNG**: L·∫ßn ch·∫°y ƒë·∫ßu ti√™n s·∫Ω m·∫•t th·ªùi gian ƒë·ªÉ download models:

```
Embedding Service: ~500MB (2-5 ph√∫t)
LLM Service: ~14GB (10-30 ph√∫t t√πy t·ªëc ƒë·ªô internet)
```

### Check status:

```bash
# Check embedding service
curl http://localhost:8002/health

# Check LLM service
curl http://localhost:8003/health

# Check main app
curl http://localhost:8000/health
```

**Khi n√†o ready?** Khi t·∫•t c·∫£ endpoints tr·∫£ v·ªÅ `status: healthy`

---

## üß™ B∆∞·ªõc 4: Test Services

### Test Embedding Service

```bash
curl -X POST http://localhost:8002/embed \
  -H "Content-Type: application/json" \
  -d '{
    "texts": ["Hello world", "„Åì„Çì„Å´„Å°„ÅØ", "Xin ch√†o"],
    "normalize": true
  }'
```

Expected response:
```json
{
  "embeddings": [[0.1, 0.2, ...], [0.3, 0.4, ...], [0.5, 0.6, ...]],
  "dimension": 384,
  "model": "paraphrase-multilingual-MiniLM-L12-v2",
  "processing_time": 0.05
}
```

### Test LLM Service

```bash
curl -X POST http://localhost:8003/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "What is a chatbot?",
    "max_new_tokens": 100,
    "temperature": 0.7
  }'
```

---

## üìö B∆∞·ªõc 5: Setup LocalStack & Knowledge Base

### Initialize LocalStack Services

```bash
# Run setup script
python scripts/setup_localstack.py
```

Script n√†y s·∫Ω:
- T·∫°o S3 buckets
- T·∫°o DynamoDB tables
- T·∫°o Secrets Manager secrets
- Setup EventBridge schedules

### Ingest Data v√†o Knowledge Base

```bash
# Run data fetcher manually
python scripts/run_data_fetcher.py

# Or trigger via LocalStack Lambda
aws --endpoint-url=http://localhost:4566 lambda invoke \
  --function-name data_fetcher \
  --payload '{}' \
  response.json
```

### Build Vector Index

```bash
# Build ChromaDB index from S3 data
python scripts/build_vector_index.py
```

---

## üéÆ B∆∞·ªõc 6: S·ª≠ D·ª•ng Chatbot

### Web UI (n·∫øu c√≥)

```bash
# M·ªü browser
http://localhost:8000
```

### API Endpoint

```bash
# Send a question
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What are the open bugs in GitLab?",
    "conversation_id": "test-123"
  }'
```

### Python Client

```python
import requests

response = requests.post(
    "http://localhost:8000/chat",
    json={
        "message": "Show me all high-priority bugs",
        "conversation_id": "session-1"
    }
)

print(response.json()["answer"])
```

---

## üîß Troubleshooting

### Problem 1: Models kh√¥ng download

```bash
# Check logs
docker-compose logs embedding-service
docker-compose logs llm-service

# Manual download (trong container)
docker exec -it chatbot-embedding bash
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')"
```

### Problem 2: Out of Memory

```bash
# Gi·∫£m resources trong docker-compose.yml
# Ho·∫∑c enable swap

# TƒÉng Docker Desktop memory limit:
# Settings ‚Üí Resources ‚Üí Memory ‚Üí 16GB+
```

### Problem 3: GPU kh√¥ng ƒë∆∞·ª£c detect

```bash
# Install NVIDIA Container Toolkit
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html

# Enable GPU in docker-compose.yml
# Uncomment d√≤ng: runtime: nvidia

# Test GPU trong container
docker exec -it chatbot-llm python -c "import torch; print(torch.cuda.is_available())"
```

### Problem 4: LocalStack connection refused

```bash
# Restart LocalStack
docker-compose restart localstack

# Check if port 4566 is free
netstat -an | grep 4566

# Re-run setup
python scripts/setup_localstack.py
```

---

## üìä Performance Tuning

### CPU Optimization

```bash
# Trong .env
EMBEDDING_BATCH_SIZE=16  # Gi·∫£m t·ª´ 32
LLM_MAX_NEW_TOKENS=256   # Gi·∫£m t·ª´ 512
```

### GPU Optimization

```bash
# Trong .env
EMBEDDING_DEVICE=cuda
LLM_DEVICE=cuda
LLM_LOAD_IN_4BIT=true  # S·ª≠ d·ª•ng quantization
```

### Caching

```bash
# Enable Redis caching
ENABLE_CACHING=true
REDIS_TTL=3600
```

---

## üßπ Cleanup & Maintenance

### Stop All Services

```bash
docker-compose down
```

### Stop and Remove Data

```bash
docker-compose down -v
```

### Remove Downloaded Models

```bash
rm -rf models-cache/*
```

### Clean Docker Images

```bash
docker system prune -a
```

---

## üìù Development Workflow

### Hot Reload (Main App)

```bash
# App code ƒë∆∞·ª£c mount v√†o container
# Ch·ªânh s·ª≠a code trong app/ ‚Üí auto reload

# Xem logs
docker-compose logs -f app
```

### Rebuild Services

```bash
# Sau khi thay ƒë·ªïi Dockerfile ho·∫∑c requirements.txt
docker-compose build embedding-service
docker-compose up -d embedding-service
```

### Run Tests

```bash
# Unit tests
docker-compose exec app pytest tests/

# Integration tests
docker-compose exec app pytest tests/integration/
```

---

## üöÄ Production Deployment (Optional)

Khi mu·ªën deploy l√™n server th·∫≠t:

1. **Thay th·∫ø LocalStack b·∫±ng AWS th·∫≠t**
```bash
LOCALSTACK_ENDPOINT=  # Leave empty to use real AWS
AWS_ACCESS_KEY_ID=your_real_key
AWS_SECRET_ACCESS_KEY=your_real_secret
```

2. **S·ª≠ d·ª•ng managed databases**
```bash
POSTGRES_HOST=your-rds-endpoint.amazonaws.com
REDIS_HOST=your-elasticache-endpoint.amazonaws.com
```

3. **Enable authentication**
```bash
ENABLE_AUTHENTICATION=true
```

4. **Use reverse proxy (Nginx)**
```bash
# Th√™m nginx service v√†o docker-compose.yml
```

---

## üìñ Next Steps

1. ‚úÖ Read [LOCAL_ARCHITECTURE.md](LOCAL_ARCHITECTURE.md) ƒë·ªÉ hi·ªÉu ki·∫øn tr√∫c
2. ‚úÖ Customize agents trong `app/agents/`
3. ‚úÖ Add custom tools trong `app/tools/`
4. ‚úÖ Integrate frontend UI
5. ‚úÖ Setup monitoring with Grafana

---

## üÜò C·∫ßn Gi√∫p ƒê·ª°?

- Check logs: `docker-compose logs -f [service-name]`
- GitHub Issues: Create an issue with logs
- Documentation: Read code comments trong `app/` v√† `services/`

---

**Happy Coding! üéâ**
